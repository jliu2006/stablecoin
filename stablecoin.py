# -*- coding: utf-8 -*-
"""Stablecoin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S041eOrcYOjP6ezrj6Ew3DrICVJRfNyR
"""

!pip install torchsde torchdiffeq
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torchsde
import torch.optim as optim
from torchdiffeq import odeint

def create_lookback_sequence(data, lookback):
    X, y = [], []
    for i in range(len(data) - lookback):
        X.append(data[i:(i + lookback)])
        y.append(data[i + lookback])
    return np.array(X), np.array(y)

def preprocess(ticker, lookback=14):
    data = yf.download(ticker).Close.values

    X, y = create_lookback_sequence(data, lookback)

    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    X_scaler = MinMaxScaler(feature_range=(1,10))
    y_scaler = MinMaxScaler(feature_range=(1,10))

    X_train_reshaped = X_train.reshape(-1, 1)
    X_test_reshaped = X_test.reshape(-1, 1)

    X_train_scaled_reshaped = X_scaler.fit_transform(X_train_reshaped)
    X_test_scaled_reshaped = X_scaler.transform(X_test_reshaped)

    X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)
    X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)

    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))
    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))

    X_train_tensor = torch.tensor(X_train_scaled.reshape(-1, lookback, 1), dtype=torch.float32)
    X_test_tensor = torch.tensor(X_test_scaled[:, :, np.newaxis], dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, X_scaler, y_scaler

train_loader, test_loader, X_scaler, y_scaler = preprocess('BUSD-USD')

import torch.nn as nn
import torchsde
import torch

class NeuralSDE(torch.nn.Module):
    def __init__(self, hidden_size, output_size):
        super(NeuralSDE, self).__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"


        self.drift_nn = nn.Sequential(
            nn.Linear(output_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

        self.diffusion_nn = nn.Sequential(
            nn.Linear(output_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def f(self, t, y):
        return self.drift_nn(y)

    def g(self, t, y):
        return self.diffusion_nn(y)


    def forward(self, ts, y0):
        return torchsde.sdeint(self, y0, ts, dt=0.01,method = 'euler')

hidden_size = 1024
output_size = 1

model = NeuralSDE(hidden_size, output_size)


print("Neural SDE model defined with f and g methods calling internal networks.")

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, targets in train_loader:
        optimizer.zero_grad()

        y0 = inputs[:, 0, :]

        ts = torch.linspace(0, 1, inputs.shape[1])

        solutions = model(ts, y0)

        predictions = solutions[-1]
        loss = criterion(predictions, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}")

print("Training finished.")

# Set the model to evaluation mode
model.eval()

# Initialize lists to store actual and predicted values
actual_values = []
predicted_values = []

# Iterate over the test data
with torch.no_grad():
    for inputs, targets in test_loader:
        # Initial state for the SDE is the first time step of each sequence
        # Ensure y0 is 2D: (batch_size, 1)
        # Use slicing and squeeze to get shape (batch_size, 1)
        y0 = inputs[:, 0, :].squeeze(1)
        # Time points for the SDE solver
        ts = torch.linspace(0, 1, inputs.shape[1])

        # Forward pass through the Neural SDE model
        solutions = model(ts, y0)

        # Get the prediction from the last time step of the solution
        predictions = solutions[-1]

        # Append actual and predicted values to the lists
        actual_values.append(targets.squeeze().numpy())
        predicted_values.append(predictions.squeeze().numpy())

# Concatenate the lists into numpy arrays
actual_values = np.concatenate(actual_values)
predicted_values = np.concatenate(predicted_values)

print("Prediction generation complete.")

import matplotlib.pyplot as plt
import numpy as np

# Inverse transform the actual and predicted values
# The scaler expects a 2D array, so we need to reshape the 1D actual_values and predicted_values
actual_values_original_scale = y_scaler.inverse_transform(actual_values.reshape(-1, 1))
predicted_values_original_scale = y_scaler.inverse_transform(predicted_values.reshape(-1, 1))

# Shift the predicted values 14 days to the left
# This means aligning the prediction at index i with the actual value at index i - 14
# We will plot predicted_values_original_scale[14:] against actual_values_original_scale[:-14]
shifted_predicted_values = predicted_values_original_scale[14:]
corresponding_actual_values = actual_values_original_scale[:-14]

plt.figure(figsize=(12, 6))
# Plot the corresponding actual values and the shifted predicted values
plt.plot(shifted_predicted_values[:, 0], label='Actual Values (Shifted)')
plt.plot(corresponding_actual_values[:, 0], label='Predicted Values (Shifted Left by 14 Days)')
plt.title('Actual vs Predicted Values (Test Set, Predicted Shifted Left)')
plt.xlabel('Time Steps')
plt.ylabel('Price')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.linear_model import LinearRegression

# Access the original data before scaling
# Assuming 'data' variable from preprocess function is accessible or can be regenerated
# If 'data' is not accessible, you might need to re-run the data download and initial preprocessing steps
try:
    # Try to use the 'data' variable if it exists
    original_prices = data
    print("Using 'data' variable for original prices.")
except NameError:
    # If 'data' is not defined, re-download the data
    print("'data' variable not found. Re-downloading data for USDT-USD.")
    ticker = 'USDT-USD' # Assuming the same ticker as used in preprocess
    original_prices = yf.download(ticker).Close.values
    print("Data re-downloaded.")


# Take the logarithm of the original prices
# Add a small epsilon to avoid log(0) if necessary
log_prices = np.log(original_prices + 1e-9)

# Reshape log_prices for use with the model and regression
# We need to create sequences similar to the training data structure to pass to the model
# However, the model.f function takes a single time step 'y' as input, not a sequence.
# The challenge is that the model.f was trained on *scaled* data. To evaluate it on *log* data,
# the model would ideally need to be trained on log prices, or we would need to scale the log prices
# using the same scaler trained on the original data range and then pass them to model.f.

# Let's try scaling the log prices using the existing scaler (y_scaler)
# Note: This might not be the most theoretically sound approach as the scaler was fit on linear prices,
# but it allows us to pass values within the expected range to the trained model.
log_prices_scaled = y_scaler.transform(log_prices.reshape(-1, 1))

# Evaluate the learned drift function (model.f) at these scaled log prices
# model.f expects input shape (batch_size, output_size)
y_eval_scaled = torch.tensor(log_prices_scaled, dtype=torch.float32)
t_eval = torch.tensor([0.0]) # Arbitrary time point
t_batch = torch.full_like(y_eval_scaled, t_eval.item()) # Match shape for batching

with torch.no_grad():
    learned_drift_on_log_scaled = model.f(t_batch, y_eval_scaled).squeeze()

# Perform linear regression on the log prices and the learned drift evaluated at scaled log prices
# The independent variable X will be the original log prices
X_log = log_prices.reshape(-1, 1)
y_drift_on_log_scaled = learned_drift_on_log_scaled.numpy().reshape(-1, 1)

linear_regressor_log = LinearRegression()
linear_regressor_log.fit(X_log, y_drift_on_log_scaled)

# Get the coefficients (slope) and intercept
slope_log = linear_regressor_log.coef_[0][0]
intercept_log = linear_regressor_log.intercept_[0]

print(f"Linear Regression Results on Log Prices:")
print(f"Slope: {slope_log:.4f}")
print(f"Intercept: {intercept_log:.4f}")

# Generate predictions from the linear regression model on log prices
predicted_drift_on_log_scaled = linear_regressor_log.predict(X_log)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X_log, y_drift_on_log_scaled, label='Learned Drift (model.f) at Scaled Log Prices', color='red', alpha=0.5, s=5)
plt.plot(X_log, predicted_drift_on_log_scaled, label='Linear Regression Fit on Log Prices', color='green', linestyle='-')

plt.xlabel('Log of Original Price')
plt.ylabel('Learned Drift (evaluated at scaled log price)')
plt.title('Linear Regression of Learned Drift on Log Prices')
plt.legend()
plt.grid(True)
plt.show()
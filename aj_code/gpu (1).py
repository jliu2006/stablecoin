# -*- coding: utf-8 -*-
"""GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mIbZ3gSy8rlK_XeB3ps2wj6n7QGVYMca
"""

import torch
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils as utils
import torchsde
import matplotlib.pyplot as plt

# --- Configuration ---
ASSET_1 = 'USDT-USD'
ASSET_2 = 'VND=X'
START_DATE = '2017-07-01'
END_DATE = '2025-07-24'
LOOKBACK = 2
TIME_STEP = 44
BATCH_SIZE = 64
TRAIN_SPLIT = 0.65
EPOCHS = 4
HIDDEN_SIZE = 1024
INPUT_FEATURES = 2
OUTPUT_SIZE = 1
LEARNING_RATE = 1e-5

# --- Data Preprocessing ---
def preprocess_data(df, lookback, time_step, batch_size=64, split_ratio=0.65):
    features = df.values
    targets = df.values[:, 1]
    features_scaler = MinMaxScaler()
    targets_scaler = MinMaxScaler()
    features_scaled = features_scaler.fit_transform(features)
    targets_scaled = targets_scaler.fit_transform(targets.reshape(-1, 1))

    X, y = [], []
    for i in range(len(features_scaled) - lookback - time_step):
        X.append(features_scaled[i:i+lookback])
        y.append(targets_scaled[i+lookback+time_step])

    X, y = np.array(X), np.array(y)
    split_idx = int(split_ratio * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)
    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)

    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, features_scaler, targets_scaler

def load_combined_data(asset1, asset2, start, end):
    data1 = yf.download(asset1, start=start, end=end)['Close']
    data2 = yf.download(asset2, start=start, end=end)['Close']
    combined = pd.concat([data1, data2], axis=1, join='outer').dropna()
    combined.columns = ['Asset1', 'Asset2']
    return combined

# --- Model Definition ---
class NeuralSDE(nn.Module):
    def __init__(self, hidden_size, input_features, output_size, lookback):
        super().__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"
        self.lookback = lookback
        # CNN layers to process the input sequence
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=input_features, out_channels=hidden_size // 2, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(in_channels=hidden_size // 2, out_channels=hidden_size, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1) # Pool to get a fixed-size vector
        )
        self.drift_linear = nn.Linear(hidden_size + output_size, output_size)
        self.diffusion_linear = nn.Linear(hidden_size + output_size, output_size)
        self._cnn_vector_buffer = None # Buffer to store the output of the CNN

    def f(self, t, y):
        if self._cnn_vector_buffer is None:
            raise RuntimeError("CNN vector buffer is not set. Call forward with input_sequence first.")
        combined = torch.cat((self._cnn_vector_buffer, y), dim=1)
        return self.drift_linear(combined)

    def g(self, t, y):
        if self._cnn_vector_buffer is None:
            raise RuntimeError("CNN vector buffer is not set. Call forward with input_sequence first.")
        combined = torch.cat((self._cnn_vector_buffer, y), dim=1)
        return self.diffusion_linear(combined)

    def forward(self, ts, y0, input_seq):
        # Permute the input sequence to match CNN's expected input shape (batch_size, channels, sequence_length)
        input_seq_permuted = input_seq.permute(0, 2, 1)
        cnn_output = self.cnn(input_seq_permuted).squeeze(-1) # Remove the last dimension after pooling
        self._cnn_vector_buffer = cnn_output
        solution = torchsde.sdeint(self, y0, ts, dt=0.01, method='euler')
        self._cnn_vector_buffer = None # Reset the buffer
        return solution

# --- Training Routine ---
def train_model(model, train_loader, criterion, optimizer, device, epochs):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device) # Move data to the correct device
            optimizer.zero_grad()
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, inputs.size(1)).to(device) # Move ts to the correct device
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            loss = criterion(preds, targets)
            loss.backward()
            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            running_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}")

# --- Evaluation Routine ---
def evaluate_model(model, test_loader, target_scaler, device, lookback):
    model.eval()
    actual, predicted = [], []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device) # Move data to the correct device
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, inputs.size(1)).to(device) # Move ts to the correct device
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            actual.append(targets.squeeze().cpu().numpy()) # Move data back to CPU for numpy
            predicted.append(preds.squeeze().cpu().numpy()) # Move data back to CPU for numpy
    actual = np.concatenate(actual)
    predicted = np.concatenate(predicted)
    actual_orig = target_scaler.inverse_transform(actual.reshape(-1, 1))
    predicted_orig = target_scaler.inverse_transform(predicted.reshape(-1, 1))
    # Align predictions for plotting
    shifted_pred = predicted_orig[:-lookback]
    actual_aligned = actual_orig[lookback:]
    return actual_aligned, shifted_pred

def plot_results(actual, predicted, lookback):
    plt.figure(figsize=(14, 7))
    plt.plot(actual, label='Actual Asset 2 Price')
    plt.plot(predicted, label=f'Predicted Asset 2 Price (Shifted by {lookback} days)')
    plt.title('Actual vs Predicted Prices (Test Set, Shifted)')
    plt.xlabel('Time Steps')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

def independent_test_on_pairs(pairs, features_scaler_combined, target_scaler_combined, model, device):
    # Visualize one pair (the second in the list)
    test_pair_np = pairs[1].cpu().numpy() if torch.is_tensor(pairs[1]) else pairs[1]
    inv_pair = features_scaler_combined.inverse_transform(test_pair_np)

    plt.figure()
    plt.plot(inv_pair[:, 1])
    plt.title("Plot of __[:, 1]")
    plt.figure()
    plt.plot(inv_pair[:, 0])
    plt.title("Plot of __[:, 0]")
    plt.show()

    # Prepare input tensor for model
    test_tensor = torch.tensor(test_pair_np, dtype=torch.float32).unsqueeze(0).to(device)
    y0 = test_tensor[:, 0, 1].unsqueeze(1)
    ts = torch.linspace(0, 1, test_tensor.shape[1]).to(device)

    model.eval()
    with torch.no_grad():
        pred = model(ts, y0, test_tensor)[-1]
        pred_orig = target_scaler_combined.inverse_transform(pred.cpu().numpy().reshape(-1, 1))
        print(pred_orig)
        print("Prediction successful for one pair.")

    predicted_values_all_pairs = []
    actual_values_all_pairs = []

    # Evaluate over all pairs
    model.eval()
    with torch.no_grad():
        for pair in pairs:
            pair = pair.to(device) # Move pair to the correct device
            pair_np = pair.cpu().numpy() if torch.is_tensor(pair) else pair
            test_tensor = torch.tensor(pair_np, dtype=torch.float32).unsqueeze(0).to(device)
            y0 = test_tensor[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, test_tensor.shape[1]).to(device)
            pred = model(ts, y0, test_tensor)[-1]
            pred_original = target_scaler_combined.inverse_transform(pred.cpu().numpy().reshape(-1, 1))[0, 0]
            actual_original = features_scaler_combined.inverse_transform(pair_np)[-1, 1]
            predicted_values_all_pairs.append(pred_original)
            actual_values_all_pairs.append(actual_original)
            print(pred_original, actual_original)

    predicted_values_all_pairs = np.array(predicted_values_all_pairs)
    actual_values_all_pairs = np.array(actual_values_all_pairs)
    mean_deviation = np.mean(predicted_values_all_pairs - actual_values_all_pairs)
    print(f"Mean deviation over all pairs: {mean_deviation:.4f}")
    mae = mean_absolute_error(actual_values_all_pairs, predicted_values_all_pairs)
    print(f"Mean Absolute Error over all pairs: {mae:.4f}")

# Example usage in your main function:
# independent_test_on_pairs(pairs, features_scaler_combined, target_scaler_combined, model, device)

# --- Main Execution ---
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("Using device:", device)
    combined_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)
    train_loader, test_loader, feat_scaler, tgt_scaler = preprocess_data(
        combined_data, LOOKBACK, TIME_STEP, BATCH_SIZE, TRAIN_SPLIT
    )
    model = NeuralSDE(HIDDEN_SIZE, INPUT_FEATURES, OUTPUT_SIZE, LOOKBACK).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    train_model(model, train_loader, criterion, optimizer, device, EPOCHS)
    actual, predicted = evaluate_model(model, test_loader, tgt_scaler, device, lookback=14)
    plot_results(actual, predicted, lookback=1)
    print("Processing complete.")

    print("Begin independent testing...")
    # independent_test_on_pairs(pairs, features_scaler_combined, target_scaler_combined, model, device)

# if __name__ == "__main__":
#     main()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)
combined_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)
train_loader, test_loader, feat_scaler, tgt_scaler = preprocess_data(
    combined_data, LOOKBACK, TIME_STEP, BATCH_SIZE, TRAIN_SPLIT
)
model = NeuralSDE(HIDDEN_SIZE, INPUT_FEATURES, OUTPUT_SIZE, LOOKBACK).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
train_model(model, train_loader, criterion, optimizer, device, EPOCHS)
actual, predicted = evaluate_model(model, test_loader, tgt_scaler, device, lookback=1)
plot_results(actual, predicted, lookback=1)
print("Processing complete.")

# Get the last 45 days of data
_ = '2025-06-07'
END_DATE = pd.to_datetime(_) - pd.Timedelta(days=2)
latest_data = load_combined_data(ASSET_1, ASSET_2, start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK)).strftime('%Y-%m-%d'), end=END_DATE)

print(latest_data.head())

latest_features = latest_data.values
latest_features_scaled = feat_scaler.transform(latest_features)

# Prepare the input for the model
X_latest = np.array([latest_features_scaled[-LOOKBACK:]])
X_latest_tensor = torch.tensor(X_latest, dtype=torch.float32).to(device)

# Make the prediction
model.eval()
with torch.no_grad():
    y0_latest = X_latest_tensor[:, 0, 1].unsqueeze(1)
    ts_latest = torch.linspace(0, 1, X_latest_tensor.size(1)).to(device)
    predicted_scaled = model(ts_latest, y0_latest, X_latest_tensor)[-1]

# Inverse transform the prediction to get the actual price
predicted_price_today = tgt_scaler.inverse_transform(predicted_scaled.cpu().numpy().reshape(-1, 1))

print(f"Predicted price for today: {predicted_price_today[0][0]:.4f}")

END_DATE = pd.to_datetime(_) + pd.Timedelta(days=44)
latest_data = load_combined_data(ASSET_1, ASSET_2, start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK+5)).strftime('%Y-%m-%d'), end=END_DATE)
latest_data

# Simulate a crash in Asset1 (stablecoin) in the latest data
# Calculate the standard deviation of Asset1 in the original combined data
END_DATE = '2025-07-05'

latest_data = load_combined_data(ASSET_1, ASSET_2, start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK)).strftime('%Y-%m-%d'), end=END_DATE)
asset1_std = latest_data['Asset1'].std()

# Create a copy of the latest data to modify
latest_data_crashed = latest_data.copy()

# Identify the index to introduce the crash (e.g., the last data point in the lookback window)
crash_index = -1 # Introduce the crash at the most recent data point

# Reduce the value of Asset1 at the crash index by 4 standard deviations
latest_data_crashed.iloc[crash_index, latest_data_crashed.columns.get_loc('Asset2')] += -4 * asset1_std

# Preprocess the crashed latest data using the same scalers
latest_features_crashed = latest_data_crashed.values
latest_features_scaled_crashed = feat_scaler.transform(latest_features_crashed) # Use the already fitted scaler

# Prepare the input for the model with the crashed data
X_latest_crashed = np.array([latest_features_scaled_crashed[-LOOKBACK:]])
X_latest_crashed_tensor = torch.tensor(X_latest_crashed, dtype=torch.float32).to(device)

# Make the prediction using the model with the crashed data
model.eval()
with torch.no_grad():
    y0_latest_crashed = X_latest_crashed_tensor[:, 0, 1].unsqueeze(1)
    ts_latest_crashed = torch.linspace(0, 1, X_latest_crashed_tensor.size(1)).to(device)
    predicted_scaled_crashed = model(ts_latest_crashed, y0_latest_crashed, X_latest_crashed_tensor)[-1]

# Inverse transform the prediction to get the actual price under the crash scenario
predicted_price_today_crashed = tgt_scaler.inverse_transform(predicted_scaled_crashed.cpu().numpy().reshape(-1, 1))

print(f"Predicted price for today after stablecoin crash: {predicted_price_today_crashed[0][0]:.4f}")

latest_data_crashed

from statsmodels.tsa.stattools import grangercausalitytests

# Load the combined data again, ensuring we have the original data for the Granger test
granger_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)

# Perform Granger causality test
# The order of columns matters: we are testing if Asset1 causes Asset2
granger_test_results = grangercausalitytests(granger_data[['Asset1', 'Asset2']], maxlag=70, verbose=True)

import requests
import pandas as pd
from datetime import datetime, timedelta
import time

def fetch_5min_candles(product_id="USDT-USD", start=None, end=None, granularity=300):
    url = f"https://api.exchange.coinbase.com/products/{product_id}/candles"
    params = {
        "start": start.isoformat(),
        "end": end.isoformat(),
        "granularity": granularity
    }
    r = requests.get(url, params=params)
    if r.status_code != 200:
        print(f"Error fetching data: {r.text}")
        return []
    return r.json()

def get_last_month_5min_data(product_id="USDT-USD"):
    granularity = 300  # 5 minutes
    max_candles = 300  # Coinbase allows max 300 candles per request
    total_minutes = 30 * 24 * 60  # 30 days
    total_candles = total_minutes // 5

    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=30)

    all_data = []

    while start_time < end_time:
        chunk_end = start_time + timedelta(minutes=granularity * max_candles / 60)
        if chunk_end > end_time:
            chunk_end = end_time

        data = fetch_5min_candles(
            product_id=product_id,
            start=start_time,
            end=chunk_end,
            granularity=granularity
        )
        all_data.extend(data)

        # Advance window
        start_time = chunk_end
        time.sleep(0.35)  # Avoid rate limits

    # Convert to DataFrame
    df = pd.DataFrame(all_data, columns=["time", "low", "high", "open", "close", "volume"])
    df["time"] = pd.to_datetime(df["time"], unit="s")
    df = df.sort_values("time").reset_index(drop=True)
    return df

# Run it
df = get_last_month_5min_data()
print(df.head())
print(df.tail())

import torch
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils as utils
import torchsde
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

# --- Configuration ---
ASSET_1 = 'USDT-USD'
ASSET_2 = 'VND=X'
START_DATE = '2017-07-01'
END_DATE = '2025-07-24'
LOOKBACK = 60  # Increased for better temporal patterns
TIME_STEP = 44
BATCH_SIZE = 64
TRAIN_SPLIT = 0.65
EPOCHS = 4
HIDDEN_SIZE = 1024  # Reduced from 1024 to mitigate overfitting
INPUT_FEATURES = 2
OUTPUT_SIZE = 1
LEARNING_RATE = 1e-5

# --- Data Preprocessing ---
def preprocess_data(df, lookback, time_step, batch_size=64, split_ratio=0.65):
    features = df[['Asset1', 'Asset2']].values # Use only Asset1 and Asset2 as features
    targets = df['Asset2'].values # Use LogReturn as target

    X_raw, y_raw = [], []
    for i in range(len(features) - lookback - time_step):
        X_raw.append(features[i:i+lookback])
        y_raw.append(targets[i+lookback+time_step])

    X_raw, y_raw = np.array(X_raw), np.array(y_raw).reshape(-1, 1)
    split_idx = int(split_ratio * len(X_raw))
    X_train_raw, X_test_raw = X_raw[:split_idx], X_raw[split_idx:]
    y_train_raw, y_test_raw = y_raw[:split_idx], y_raw[split_idx:]

    feat_scaler = MinMaxScaler()
    tgt_scaler = MinMaxScaler()

    # Scale features and targets separately
    X_train = feat_scaler.fit_transform(X_train_raw.reshape(-1, features.shape[1])).reshape(X_train_raw.shape)
    X_test = feat_scaler.transform(X_test_raw.reshape(-1, features.shape[1])).reshape(X_test_raw.shape)

    y_train = tgt_scaler.fit_transform(y_train_raw)
    y_test = tgt_scaler.transform(y_test_raw)

    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)
    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)

    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, feat_scaler, tgt_scaler, X_test


def load_combined_data(asset1, asset2, start, end):
    data1 = yf.download(asset1, start=start, end=end)['Close']
    data2 = yf.download(asset2, start=start, end=end)['Close']
    combined = pd.concat([data1, data2], axis=1, join='outer').dropna()
    combined.columns = ['Asset1', 'Asset2']
    return combined


# --- Model Definition ---
class NeuralSDE(nn.Module):
    def __init__(self, hidden_size, input_features, output_size, lookback):
        super().__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"
        self.lookback = lookback
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=input_features, out_channels=hidden_size // 2, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(in_channels=hidden_size // 2, out_channels=hidden_size, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        self.drift_linear = nn.Linear(hidden_size + output_size, output_size)
        self.diffusion_linear = nn.Linear(hidden_size + output_size, output_size)
        self._cnn_vector_buffer = None

    def f(self, t, y):
        if self._cnn_vector_buffer is None:
            raise RuntimeError("CNN vector buffer is not set. Call forward with input_sequence first.")
        combined = torch.cat((self._cnn_vector_buffer, y), dim=1)
        return self.drift_linear(combined)

    def g(self, t, y):
        if self._cnn_vector_buffer is None:
            raise RuntimeError("CNN vector buffer is not set. Call forward with input_sequence first.")
        combined = torch.cat((self._cnn_vector_buffer, y), dim=1)
        return self.diffusion_linear(combined)

    def forward(self, ts, y0, input_seq):
        input_seq_permuted = input_seq.permute(0, 2, 1)
        cnn_output = self.cnn(input_seq_permuted).squeeze(-1)
        self._cnn_vector_buffer = cnn_output
        solution = torchsde.sdeint(self, y0, ts, dt=0.01, method='euler')
        self._cnn_vector_buffer = None
        return solution


# --- Training Routine ---
def train_model(model, train_loader, criterion, optimizer, device, epochs):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, inputs.size(1)).to(device)
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            loss = criterion(preds, targets)
            loss.backward()
            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            running_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}")


# --- Evaluation Routine ---
def evaluate_model(model, test_loader, target_scaler, device, lookback):
    model.eval()
    actual, predicted = [], []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, inputs.size(1)).to(device)
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            actual.append(targets.squeeze().cpu().numpy())
            predicted.append(preds.squeeze().cpu().numpy())
    actual = np.concatenate(actual)
    predicted = np.concatenate(predicted)
    actual_orig = target_scaler.inverse_transform(actual.reshape(-1, 1))
    predicted_orig = target_scaler.inverse_transform(predicted.reshape(-1, 1))
    return actual_orig, predicted_orig


def plot_results(actual, predicted):
    plt.figure(figsize=(14, 7))
    plt.plot(actual, label='Actual Log-Return')
    plt.plot(predicted, label='Predicted Log-Return')
    plt.title('Actual vs Predicted Log-Returns (Test Set)')
    plt.xlabel('Time Steps')
    plt.ylabel('Log-Return')
    plt.legend()
    plt.grid(True)
    plt.show()


# --- Main Execution ---
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("Using device:", device)
    combined_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)
    train_loader, test_loader, feat_scaler, tgt_scaler, _ = preprocess_data(
        combined_data, LOOKBACK, TIME_STEP, BATCH_SIZE, TRAIN_SPLIT
    )
    model = NeuralSDE(HIDDEN_SIZE, INPUT_FEATURES, OUTPUT_SIZE, LOOKBACK).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    train_model(model, train_loader, criterion, optimizer, device, EPOCHS)
    actual, predicted = evaluate_model(model, test_loader, tgt_scaler, device, lookback=LOOKBACK)
    plot_results(actual, predicted)
    print("Processing complete.")


if __name__ == "__main__":
    main()

# Simulate a crash in Asset1 (stablecoin) in the latest data
# Calculate the standard deviation of Asset1 in the original combined data
import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")
START_DATE = '2025-1-05'
END_DATE = '2025-07-05'
date_range = pd.date_range(start=START_DATE, end=END_DATE, freq='D')
differences = []

for end_date in date_range:
    # Load data window ending at current end_date with length LOOKBACK
    latest_data = load_combined_data(
        ASSET_1, ASSET_2,
        start=(pd.to_datetime(end_date) - pd.Timedelta(days=LOOKBACK)).strftime('%Y-%m-%d'),
        end=end_date.strftime('%Y-%m-%d')
    )
    asset1_std = latest_data['Asset1'].std()

    # Prepare two versions of data: no shock (0 std) and crash (4 std)
    latest_data_no_shock = latest_data.copy()
    latest_data_shock = latest_data.copy()

    # Apply 4 std crash on the last day in shock version
    crash_index = -1
    latest_data_shock.iloc[crash_index, latest_data_shock.columns.get_loc('Asset1')] -= 30 * asset1_std

    # Scale features
    no_shock_scaled = feat_scaler.transform(latest_data_no_shock.values)
    shock_scaled = feat_scaler.transform(latest_data_shock.values)

    # Prepare tensors
    X_no_shock = torch.tensor(no_shock_scaled[-LOOKBACK:], dtype=torch.float32).unsqueeze(0).to(device)
    X_shock = torch.tensor(shock_scaled[-LOOKBACK:], dtype=torch.float32).unsqueeze(0).to(device)

    # Predict no shock
    model.eval()
    with torch.no_grad():
        y0_no_shock = X_no_shock[:, 0, 1].unsqueeze(1)
        ts = torch.linspace(0, 1, X_no_shock.size(1)).to(device)
        pred_no_shock_scaled = model(ts, y0_no_shock, X_no_shock)[-1]
        pred_no_shock = tgt_scaler.inverse_transform(pred_no_shock_scaled.cpu().numpy().reshape(-1, 1))[0, 0]

    # Predict with shock
    with torch.no_grad():
        y0_shock = X_shock[:, 0, 1].unsqueeze(1)
        pred_shock_scaled = model(ts, y0_shock, X_shock)[-1]
        pred_shock = tgt_scaler.inverse_transform(pred_shock_scaled.cpu().numpy().reshape(-1, 1))[0, 0]

    diff = pred_shock - pred_no_shock
    differences.append(diff)

    print(f"Date: {end_date.strftime('%Y-%m-%d')}, Price difference (4 std - 0 std): {diff:.4f}")

# Optionally, print average difference over all dates:
avg_diff = np.mean(differences)
print(f"\nAverage signed difference over all dates: {avg_diff:.4f}")

# Simulate a crash in Asset1 (stablecoin) in the latest data
# Calculate the standard deviation of Asset1 in the original combined data
import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

# Use a single END_DATE for repeated predictions
END_DATE = '2025-07-05'
num_iterations = 100  # Number of times to repeat the prediction
differences = []

# Load the data window once
latest_data = load_combined_data(
    ASSET_1, ASSET_2,
    start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK)).strftime('%Y-%m-%d'),
    end=END_DATE
)

# Calculate the standard deviation of Asset1 in this specific data window
asset1_std = latest_data['Asset1'].std()

for i in range(num_iterations):
    # Prepare two versions of data: no shock (0 std) and crash (4 std)
    latest_data_no_shock = latest_data.copy()
    latest_data_shock = latest_data.copy()

    # Apply 30 std crash on the last day in shock version (as in the original code)
    crash_index = -1
    latest_data_shock.iloc[crash_index, latest_data_shock.columns.get_loc('Asset1')] -= 30 * asset1_std

    # Scale features using the pre-fitted scaler
    no_shock_scaled = feat_scaler.transform(latest_data_no_shock.values)
    shock_scaled = feat_scaler.transform(latest_data_shock.values)

    # Prepare tensors
    X_no_shock = torch.tensor(no_shock_scaled[-LOOKBACK:], dtype=torch.float32).unsqueeze(0).to(device)
    X_shock = torch.tensor(shock_scaled[-LOOKBACK:], dtype=torch.float32).unsqueeze(0).to(device)

    # Predict no shock
    model.eval()
    with torch.no_grad():
        y0_no_shock = X_no_shock[:, 0, 1].unsqueeze(1)
        ts = torch.linspace(0, 1, X_no_shock.size(1)).to(device)
        pred_no_shock_scaled = model(ts, y0_no_shock, X_no_shock)[-1]
        pred_no_shock = tgt_scaler.inverse_transform(pred_no_shock_scaled.cpu().numpy().reshape(-1, 1))[0, 0]

    # Predict with shock
    with torch.no_grad():
        y0_shock = X_shock[:, 0, 1].unsqueeze(1)
        pred_shock_scaled = model(ts, y0_shock, X_shock)[-1]
        pred_shock = tgt_scaler.inverse_transform(pred_shock_scaled.cpu().numpy().reshape(-1, 1))[0, 0]

    diff = pred_shock - pred_no_shock
    differences.append(diff)

    print(f"Iteration {i+1}/{num_iterations}, Price difference (Shock - No Shock): {diff:.4f}")

# Optionally, print average difference over all iterations:
avg_diff = np.mean(differences)
print(f"\nAverage signed difference over {num_iterations} iterations: {avg_diff:.4f}")
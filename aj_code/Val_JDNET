import torch
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils as utils
import matplotlib.pyplot as plt
import random

# Set random seed
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# --- Configuration ---
ASSET_1 = 'BTC-USD'
ASSET_2 = 'USDT-USD'

START_DATE = '2017-07-01'
END_DATE = '2025-07-24'
LOOKBACK = 44
TIME_STEP = 1
BATCH_SIZE = 256
TRAIN_SPLIT = 0.65
EPOCHS = 6
HIDDEN_SIZE = 1024
INPUT_FEATURES = 2
OUTPUT_SIZE = 1
LEARNING_RATE = 1e-4

# --- Data Preprocessing ---
def preprocess_data(df, lookback, time_step, batch_size=64, split_ratio=0.65):
    features = df[['Asset1', 'Asset2']].values
    targets = df['Asset2'].values

    X_raw, y_raw = [], []
    for i in range(len(features) - lookback - time_step):
        X_raw.append(features[i:i+lookback])
        y_raw.append(targets[i+lookback+time_step])

    X_raw, y_raw = np.array(X_raw), np.array(y_raw).reshape(-1, 1)
    total_len = len(X_raw)
    train_idx = int(split_ratio * total_len)
    val_idx = int((split_ratio + (1 - split_ratio)/2) * total_len)

    X_train_raw = X_raw[:train_idx]
    y_train_raw = y_raw[:train_idx]
    X_val_raw = X_raw[train_idx:val_idx]
    y_val_raw = y_raw[train_idx:val_idx]
    X_test_raw = X_raw[val_idx:]
    y_test_raw = y_raw[val_idx:]

    feat_scaler = MinMaxScaler()
    tgt_scaler = MinMaxScaler()

    X_train = feat_scaler.fit_transform(X_train_raw.reshape(-1, features.shape[1])).reshape(X_train_raw.shape)
    X_val = feat_scaler.transform(X_val_raw.reshape(-1, features.shape[1])).reshape(X_val_raw.shape)
    X_test = feat_scaler.transform(X_test_raw.reshape(-1, features.shape[1])).reshape(X_test_raw.shape)

    y_train = tgt_scaler.fit_transform(y_train_raw)
    y_val = tgt_scaler.transform(y_val_raw)
    y_test = tgt_scaler.transform(y_test_raw)

    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_val = torch.tensor(X_val, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    y_val = torch.tensor(y_val, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32)

    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader, feat_scaler, tgt_scaler, X_test

# --- Load Data ---
def load_combined_data(asset1, asset2, start, end):
    data1 = yf.download(asset1, start=start, end=end)['Close']
    data2 = yf.download(asset2, start=start, end=end)['Close']
    combined = pd.concat([data1, data2], axis=1).dropna()
    combined.columns = ['Asset1', 'Asset2']
    return combined

# --- Model ---
class NeuralSDEWithJump(nn.Module):
    def __init__(self, hidden_size, input_features, output_size, lookback):
        super().__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"
        self.lookback = lookback

        self.drift_lstm = nn.LSTM(input_features, hidden_size, batch_first=True)
        self.diffusion_lstm = nn.LSTM(input_features, hidden_size, batch_first=True)

        self.drift_linear = nn.Linear(hidden_size + output_size, output_size)
        self.diffusion_linear = nn.Linear(hidden_size + output_size, output_size)

        self.jump_intensity = 0.000005
        self.jump_mean = 0.0
        self.jump_std = 0.05
        self._lstm_hidden_buffer = None

    def f(self, t, y):
        if self._lstm_hidden_buffer is None:
            raise RuntimeError("LSTM hidden state not set.")
        combined = torch.cat((self._lstm_hidden_buffer, y), dim=1)
        return self.drift_linear(combined)

    def g(self, t, y):
        if self._lstm_hidden_buffer is None:
            raise RuntimeError("LSTM hidden state not set.")
        combined = torch.cat((self._lstm_hidden_buffer, y), dim=1)
        return self.diffusion_linear(combined)

    def jump(self, t, y):
        jump_prob = self.jump_intensity * 0.01
        jump_occurred = torch.bernoulli(torch.full_like(y, jump_prob))
        jump_noise = torch.randn_like(y) * self.jump_std + self.jump_mean
        return jump_occurred * jump_noise

    def forward(self, ts, y0, input_seq):
        lstm_output, (hidden, _) = self.drift_lstm(input_seq)
        self._lstm_hidden_buffer = hidden.squeeze(0)

        y = y0
        ys = [y0]
        dt = ts[1] - ts[0]

        for t in ts[1:]:
            drift = self.f(t, y)
            diffusion = self.g(t, y)
            brownian = torch.randn_like(y) * torch.sqrt(dt)
            jump = self.jump(t, y)
            y = y + drift * dt + diffusion * brownian + jump
            ys.append(y)

        ys = torch.stack(ys)
        self._lstm_hidden_buffer = None
        return ys

# --- Training ---
def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):
    model.train()
    val_losses = []

    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.tensor([0.0, 1.0], device=device)
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            loss = criterion(preds, targets)
            loss.backward()
            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            running_loss += loss.item()

        # --- Validation ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for val_inputs, val_targets in val_loader:
                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)
                y0 = val_inputs[:, 0, 1].unsqueeze(1)
                ts = torch.tensor([0.0, 1.0], device=device)
                val_preds = model(ts, y0, val_inputs)[-1]
                val_loss += criterion(val_preds, val_targets).item()
        model.train()

        avg_train_loss = running_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
    
    return val_losses

# --- Evaluation ---
def evaluate_model(model, test_loader, target_scaler, device, lookback):
    model.eval()
    actual, predicted = [], []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.tensor([0.0, 1.0], device=device)
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            actual.append(targets.squeeze().detach().cpu().numpy())
            predicted.append(preds.squeeze().detach().cpu().numpy())
    actual = np.concatenate(actual)
    predicted = np.concatenate(predicted)
    actual_orig = target_scaler.inverse_transform(actual.reshape(-1, 1))
    predicted_orig = target_scaler.inverse_transform(predicted.reshape(-1, 1))
    shifted_pred = predicted_orig[:-lookback]
    actual_aligned = actual_orig[lookback:]
    return actual_aligned, shifted_pred

def plot_results(actual, predicted, lookback):
    plt.figure(figsize=(14, 7))
    plt.plot(actual, label='Actual Asset 2')
    plt.plot(predicted, label=f'Predicted Asset 2 (shift={lookback})')
    plt.title('Actual vs Predicted')
    plt.xlabel('Time Steps')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.show()

    from sklearn.metrics import mean_squared_error
    rmse = np.sqrt(mean_squared_error(actual, predicted))
    print(f"Test RMSE: {rmse:.4f}")

# --- Validation Loss Plot ---
def plot_val_loss(val_losses):
    plt.figure(figsize=(10, 5))
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.title('Validation Loss Over Epochs')
    plt.grid(True)
    plt.legend()
    plt.show()

# --- Shock Simulation ---
def simulate_shock_and_predict(model, feat_scaler, tgt_scaler, combined_data, lookback, device, std_multiplier=-2):
    model.eval()
    window = combined_data[['Asset1', 'Asset2']].values[-lookback:].copy()
    asset1_std = combined_data['Asset1'].std()
    shocked_value = window[-1, 0] + std_multiplier * asset1_std
    print(f"Original Asset1: {window[-1, 0]:.2f}, Shocked: {shocked_value:.2f}")
    window[-1, 0] = shocked_value
    window_scaled = feat_scaler.transform(window)
    input_tensor = torch.tensor(window_scaled, dtype=torch.float32).unsqueeze(0).to(device)
    y0 = input_tensor[:, 0, 1].unsqueeze(1)
    ts = torch.tensor([0.0, 1.0], device=device)
    with torch.no_grad():
        output = model(ts, y0, input_tensor)
        predicted_scaled = output[-1].cpu().numpy().reshape(-1, 1)
        predicted = tgt_scaler.inverse_transform(predicted_scaled)
    print(f"Predicted Asset2 after shock: {predicted[0][0]:.4f}")
    return predicted[0][0]

# --- Main Execution ---
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("Using device:", device)

    combined_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)
    train_loader, val_loader, test_loader, feat_scaler, tgt_scaler, _ = preprocess_data(
        combined_data, LOOKBACK, TIME_STEP, BATCH_SIZE, TRAIN_SPLIT
    )

    model = NeuralSDEWithJump(HIDDEN_SIZE, INPUT_FEATURES, OUTPUT_SIZE, LOOKBACK).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS)
    plot_val_loss(val_losses)

    actual, predicted = evaluate_model(model, test_loader, tgt_scaler, device, lookback=LOOKBACK)
    plot_results(actual, predicted, lookback=LOOKBACK)

    print("Simulating shock...")
    simulate_shock_and_predict(model, feat_scaler, tgt_scaler, combined_data, LOOKBACK, device, std_multiplier=-2)
    simulate_shock_and_predict(model, feat_scaler, tgt_scaler, combined_data, LOOKBACK, device, std_multiplier=4)
    print("Done.")

# Run
if __name__ == "__main__":
    main()
simulate_shock_and_predict(model, feat_scaler, tgt_scaler, combined_data, LOOKBACK, device, std_multiplier=-2)
simulate_shock_and_predict(model, feat_scaler, tgt_scaler, combined_data, LOOKBACK, device, std_multiplier=4)

# -*- coding: utf-8 -*-
"""Copy of Main_A_GPU_Ready.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16YO68deJjzUOktg_qxQF5fHNuS7iTFxE
"""

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
# !pip install torchsde
import numpy as np
import pandas as pd
import yfinance as yf
import pywt
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils as utils
from torch.utils.data import TensorDataset,DataLoader
import torchsde
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt

# Parameters
ticker = 'AAPL'
start_date = '2022-01-01'
end_date = '2023-01-01'
num_simulations = 100
N = 100  # Total time steps
dt = 1 / 252  # Daily time steps
shock_index = 45
shock_magnitude = -100  # In standard deviations

# Fetch historical data
data = yf.download(ticker, start=start_date, end=end_date)['Close'].dropna()
log_returns = np.log(data / data.shift(1)).dropna()

# Estimate parameters
mu = log_returns.mean()
sigma = log_returns.std()
S0 = data.iloc[-1]  # Start from the latest price

# Function to simulate GBM
def simulate_gbm(S0, mu, sigma, N, dt, num_simulations, shock_index, shock_magnitude):
    # Ensure S0, mu, and sigma are NumPy scalars
    S0_scalar = np.array(S0).item() if isinstance(S0, (pd.Series, pd.DataFrame)) else S0
    mu_scalar = np.array(mu).item() if isinstance(mu, (pd.Series, pd.DataFrame)) else mu
    sigma_scalar = np.array(sigma).item() if isinstance(sigma, (pd.Series, pd.DataFrame)) else sigma

    simulations = np.zeros((N, num_simulations))
    simulations[0, :] = S0_scalar

    for i in range(1, N):
        Z = np.random.normal(size=num_simulations)
        if i == shock_index:
            Z += shock_magnitude  # Apply shock here
        simulations[i, :] = simulations[i-1, :] * np.exp((mu_scalar - 0.5 * sigma_scalar**2) * dt + sigma_scalar * np.sqrt(dt) * Z)

    return simulations

# Run simulations
simulated_paths = simulate_gbm(S0, mu, sigma, N, dt, num_simulations, shock_index, shock_magnitude)

# Plot some of the simulated paths
plt.figure(figsize=(12, 6))
for i in range(10):
    plt.plot(simulated_paths[:, i], alpha=0.6)
plt.axvline(x=shock_index, color='red', linestyle='--', label='Shock at t=45')
plt.title(f"{ticker} GBM Simulations with 100Ïƒ Shock at Index {shock_index}")
plt.xlabel("Time Step")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.show()

def simulate_ou(S0: float, mu: float, theta: float, sigma: float, num_simulations: int, N: int, DT: float) -> np.ndarray:
    prices = np.zeros((N, num_simulations))
    prices[0, :] = S0
    for i in range(1, N):
        Z = np.random.normal(size=num_simulations)
        prices[i, :] = (
            prices[i - 1, :] +
            theta * (mu - prices[i - 1, :]) * DT +
            sigma * np.sqrt(DT) * Z
        )
    return prices
def simulate_gbm(S0: float, mu: float, sigma: float, num_simulations: int, N: int, DT: float) -> np.ndarray:
    prices = np.zeros((N, num_simulations))
    prices[0, :] = S0

    for i in range(1, N):
        Z = np.random.standard_normal(num_simulations)
        drift = (mu - 0.5 * sigma ** 2) * DT
        diffusion = sigma * np.sqrt(DT) * Z
        prices[i, :] = prices[i - 1, :] * np.exp(drift + diffusion)

    return prices


def apply_direct_shock(paths, shock_multiplier=500, shock_duration=1):
    shocked_paths = np.copy(paths)
    shock_indices = []

    for sim_idx in range(paths.shape[1]):
        # Select index after SHOCK_START_INDEX with enough room to apply the shock
        possible_indices = np.arange(SHOCK_START_INDEX, paths.shape[0] - shock_duration - 1)
        if len(possible_indices) == 0:
            continue

        shock_index = np.random.choice(possible_indices)

        # Estimate std_dev from previous stable window
        std_window = shocked_paths[max(0, shock_index - 10):shock_index, sim_idx]
        if std_window.shape[0] < 2:
            continue
        std_dev = np.std(std_window)

        # Apply shock by multiplying the price at the shock index
        shock_factor = abs(1 + shock_multiplier * (std_dev / shocked_paths[shock_index, sim_idx]) * np.random.choice([-1, 1]))
        shocked_paths[shock_index, sim_idx] *= shock_factor

        # For the duration of the shock, maintain the shocked value
        if shock_duration > 1:
             shocked_paths[shock_index + 1:shock_index + shock_duration, sim_idx] = shocked_paths[shock_index, sim_idx]

        shock_indices.append(shock_index)

    return shocked_paths, shock_indices


def reconstruct_paths(modified_cA, modified_cD, original_length):
    paths = []
    for cA, cD in zip(modified_cA, modified_cD):
        rec = pywt.idwt(cA, cD, WAVELET)
        rec = rec[:original_length] if len(rec) > original_length else np.pad(rec, (0, original_length - len(rec)), 'edge')
        paths.append(rec)
    return np.array(paths).T

def plot_paths(t, original_paths, reconstructed_paths, num_paths=5, title='Original vs Reconstructed (Shocked) Paths'):
    plt.figure(figsize=(14, 7))
    for i in range(num_paths):
        plt.plot(t, original_paths[:, i], '--', alpha=0.7, label=f'Original {i+1}' if i == 0 else "_nolegend_")
        plt.plot(t, reconstructed_paths[:, i], '-', alpha=0.7, label=f'Reconstructed {i+1}' if i == 0 else "_nolegend_")
    plt.title(title)
    plt.xlabel('Time (Years)')
    plt.ylabel('Price ($)')
    plt.legend()
    plt.grid(True)
    plt.show()
def preprocess_combined(data, lookback, time_step, batch_size=64, split_ratio=0.65):
    # Extract features and target
    features = data.values[:, :]
    target = data.values[:, 1]

    # Initialize scalers
    features_scaler = MinMaxScaler(feature_range=(0, 1))
    target_scaler = MinMaxScaler(feature_range=(0, 1))

    # Scale the data
    features_scaled = features_scaler.fit_transform(features)
    target_scaled = target_scaler.fit_transform(target.reshape(-1, 1))

    # Create lookback sequences
    X, y = [], []
    for i in range(len(features_scaled) - lookback - time_step):
        X.append(features_scaled[i:(i + lookback)])
        y.append(target_scaled[i + lookback + time_step])

    X = np.array(X)
    y = np.array(y)

    # Split into train and test
    split = int(split_ratio * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

    # Create datasets and loaders
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, features_scaler, target_scaler

t1 = 'USDT-USD'
t2 = 'VND=X'
START_DATE = '2018-01-01'
END_DATE = '2025-07-02'
ou_data = yf.download(t1, start=START_DATE, end=END_DATE)['Close'].rolling(window=60).mean()

gm_data = yf.download(t2,  start=START_DATE, end=END_DATE)['Close'].rolling(window=1).mean()

combined_data = pd.concat((ou_data, gm_data), axis=1, join='outer')


combined_data.dropna(inplace=True)
display(combined_data.head())

T = 1
DT = 1 / 252
N = int(T / DT)
t = np.linspace(0, T, N)

N_simulations = 100
SHOCK_STD_MULTIPPLIER = 50
SHOCK_DURATION = 1
WAVELET = 'db2'
LOOKBACK = 42
SHOCK_START_INDEX = 45
WINDOW_BEFORE = LOOKBACK
WINDOW_AFTER = SHOCK_DURATION + 10

lookback = 45
time_step = 1
train_loader_combined, test_loader_combined, features_scaler_combined, target_scaler_combined = preprocess_combined(combined_data, lookback = lookback, time_step = time_step)

print("Preprocessing with combined data complete.")

usdt_data = ou_data
usdt_returns = usdt_data.pct_change().dropna()
usdt_sigma = usdt_returns.std() * np.sqrt(252)
usdt_S0 = 1
usdt_mu = 1.0
usdt_theta = 180.0

print(f"USDT OU Parameters: mu={usdt_mu:.4f}, theta={usdt_theta:.4f}, sigma={usdt_sigma.iloc[0]:.4f}")
usdt_paths = simulate_ou(usdt_S0, usdt_mu, usdt_theta, float(usdt_sigma.iloc[0]), N_simulations, N, DT)

# Apply the shock using the modified function
shocked_usdt_paths, usdt_shock_indices = apply_direct_shock(usdt_paths, shock_multiplier=SHOCK_STD_MULTIPPLIER, shock_duration=SHOCK_DURATION)
reconstructed_usdt_paths = shocked_usdt_paths # Since we are applying a direct shock, the shocked paths are the reconstructed paths

print("Reconstruction of shocked USDT paths complete.")

rub_data = gm_data
rub_log_returns = np.log(rub_data / rub_data.shift(1)).dropna()
rub_mu = rub_log_returns.mean() * 252
rub_sigma = rub_log_returns.std() * np.sqrt(252)
rub_S0 = rub_data.mean()

print(f"RUB GBM Parameters: mu={rub_mu.iloc[0]:.4f}, sigma={rub_sigma.iloc[0]:.4f}")
# Simulate rub_paths for the number of *successful* shocks
rub_paths = simulate_gbm(rub_S0, float(rub_mu.iloc[0]), float(rub_sigma.iloc[0]), len(usdt_shock_indices), N, DT)

pairs = [
    np.column_stack((
        reconstructed_usdt_paths[shock_idx - WINDOW_BEFORE:shock_idx + WINDOW_AFTER, i],
        rub_paths[shock_idx - WINDOW_BEFORE:shock_idx + WINDOW_AFTER, i]
    ))
    for i, shock_idx in enumerate(usdt_shock_indices)
    if shock_idx - WINDOW_BEFORE >= 0 and shock_idx + WINDOW_AFTER <= reconstructed_usdt_paths.shape[0]
]
original_pairs = np.array(pairs)
pairs = [features_scaler_combined.fit_transform(pair) for pair in pairs]
pairs = torch.tensor(np.array(pairs),dtype = torch.float32)

plt.plot(shocked_usdt_paths[:,10])

class NeuralSDE(torch.nn.Module):
    def __init__(self, hidden_size, input_features, output_size, lookback):
        super(NeuralSDE, self).__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"
        self.lookback = lookback
        self.input_features = input_features
        self.output_size = output_size
        self.drift_lstm = nn.LSTM(input_features, hidden_size, batch_first=True)
        self.diffusion_lstm = nn.LSTM(input_features, hidden_size, batch_first=True)

        self.drift_linear = nn.Linear(hidden_size + output_size, output_size)
        self.diffusion_linear = nn.Linear(hidden_size + output_size, output_size)

        self.register_buffer('_lstm_hidden_buffer', None)
    def f(self, t, y):
        lstm_hidden = self._lstm_hidden_buffer
        if lstm_hidden is None:
             raise RuntimeError("LSTM hidden state buffer is not set. Call forward with input_sequence first.")

        combined_input = torch.cat((lstm_hidden, y), dim=1)

        return self.drift_linear(combined_input)
    def g(self, t, y):
        lstm_hidden = self._lstm_hidden_buffer
        if lstm_hidden is None:
             raise RuntimeError("LSTM hidden state buffer is not set. Call forward with input_sequence first.")

        combined_input = torch.cat((lstm_hidden, y), dim=1)

        return self.diffusion_linear(combined_input)


    def forward(self, ts, y0, input_sequence):
        lstm_output, (lstm_hidden, _) = self.drift_lstm(input_sequence)
        self._lstm_hidden_buffer = lstm_hidden.squeeze(0)

        solutions = torchsde.sdeint(self, y0, ts, dt=0.01, method='euler')

        self._lstm_hidden_buffer = None

        return solutions

hidden_size = 1024
input_features = 2
output_size = 1

model = NeuralSDE(hidden_size, input_features, output_size, lookback)

print("Neural SDE model defined with LSTM layers only.")
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-5)

num_epochs = 5

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, targets in train_loader_combined:
        optimizer.zero_grad()
        y0 = inputs[:, 0, 1].unsqueeze(1)

        ts = torch.linspace(0, 1, inputs.shape[1])

        solutions = model(ts, y0, inputs)

        predictions = solutions[-1]

        loss = criterion(predictions, targets)
        print(loss)
        loss.backward()
        utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader_combined)}")

print("Training finished.")
model.eval()

actual_values_combined = []
predicted_values_combined = []

with torch.no_grad():
    for inputs, targets in test_loader_combined:
        y0 = inputs[:, 0, 1].unsqueeze(1)

        ts = torch.linspace(0, 1, inputs.shape[1])

        solutions = model(ts, y0, inputs)

        predictions = solutions[-1]

        actual_values_combined.append(targets.squeeze().numpy())
        predicted_values_combined.append(predictions.squeeze().numpy())

actual_values_combined = np.concatenate(actual_values_combined)
predicted_values_combined = np.concatenate(predicted_values_combined)

print("Prediction generation complete for combined data.")
lookback = 14

actual_values_original_scale_combined = target_scaler_combined.inverse_transform(actual_values_combined.reshape(-1, 1))
predicted_values_original_scale_combined = target_scaler_combined.inverse_transform(predicted_values_combined.reshape(-1, 1))

shifted_predicted_values = predicted_values_original_scale_combined[:-lookback]
corresponding_actual_values = actual_values_original_scale_combined[lookback:]

plt.figure(figsize=(14, 7))
plt.plot(corresponding_actual_values, label='Actual Ruble Price')
plt.plot(shifted_predicted_values, label=f'Predicted Ruble Price (Shifted Left by {lookback} Days)')
plt.title('Actual vs Predicted Ruble Prices (Test Set, Shifted)')
plt.xlabel('Time Steps')
plt.ylabel('Ruble Price')
plt.legend()
plt.grid(True)
plt.show()

test_pair_np = pairs[1]
__ = features_scaler_combined.inverse_transform(test_pair_np)
plt.figure()
plt.plot(__[:, 1])
plt.title("Plot of __[:, 1]")

# Second plot
plt.figure()
plt.plot(__[:, 0])
plt.title("Plot of __[:, 0]")

plt.show()
test_tensor = torch.tensor(test_pair_np, dtype=torch.float32).unsqueeze(0)

y0 = test_tensor[:, 0, 1].unsqueeze(1)

ts = torch.linspace(0, 1, test_tensor.shape[1])

pred = model(ts, y0, test_tensor)[-1]
print(target_scaler_combined.inverse_transform(pred.detach().numpy().reshape(-1,1)))
print("Prediction successful for one pair.")

from sklearn.metrics import mean_absolute_error

predicted_values_all_pairs = []
actual_values_all_pairs = []

model.eval()
with torch.no_grad():
    for pair in pairs:
        # Shape: (1, seq_len, num_features)
        test_tensor = pair.unsqueeze(0)

        # Initial condition for the model (2nd column at t=0)
        y0 = test_tensor[:, 0, 1].unsqueeze(1)

        # Time steps
        ts = torch.linspace(0, 1, test_tensor.shape[1])

        # Predict the entire trajectory and take the last time point
        pred = model(ts, y0, test_tensor)[-1]  # shape: (1,)

        # Inverse transform the predicted and actual values
        pred_original = target_scaler_combined.inverse_transform(pred.cpu().numpy().reshape(-1, 1))[0, 0]
        actual_original = features_scaler_combined.inverse_transform(pair.cpu().numpy())[-1, 1]

        predicted_values_all_pairs.append(pred_original)
        actual_values_all_pairs.append(actual_original)
        print(pred_original, actual_original)

# Convert to numpy arrays
predicted_values_all_pairs = np.array(predicted_values_all_pairs)
actual_values_all_pairs = np.array(actual_values_all_pairs)

# Compute the mean deviation
mean_deviation = np.mean(predicted_values_all_pairs - actual_values_all_pairs)

print(f"Mean deviation over all pairs: {mean_deviation:.4f}")
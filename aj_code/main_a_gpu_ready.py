# -*- coding: utf-8 -*-
"""GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mIbZ3gSy8rlK_XeB3ps2wj6n7QGVYMca
"""

import torch
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils as utils
import torchsde
import matplotlib.pyplot as plt

# --- Configuration ---
ASSET_1 = 'USDT-USD'
ASSET_2 = 'VND=X'
START_DATE = '2017-07-01'
END_DATE = '2025-07-24'
LOOKBACK = 2
TIME_STEP = 44
BATCH_SIZE = 64
TRAIN_SPLIT = 0.65
EPOCHS = 4
HIDDEN_SIZE = 1024
INPUT_FEATURES = 2
OUTPUT_SIZE = 1
LEARNING_RATE = 1e-5

# --- Data Preprocessing ---
def preprocess_data(df, lookback, time_step, batch_size=64, split_ratio=0.65):
    features = df.values
    targets = df.values[:, 1]
    features_scaler = MinMaxScaler()
    targets_scaler = MinMaxScaler()
    features_scaled = features_scaler.fit_transform(features)
    targets_scaled = targets_scaler.fit_transform(targets.reshape(-1, 1))

    X, y = [], []
    for i in range(len(features_scaled) - lookback - time_step):
        X.append(features_scaled[i:i+lookback])
        y.append(targets_scaled[i+lookback+time_step])

    X, y = np.array(X), np.array(y)
    split_idx = int(split_ratio * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)
    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)

    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, features_scaler, targets_scaler

def load_combined_data(asset1, asset2, start, end):
    data1 = yf.download(asset1, start=start, end=end)['Close']
    data2 = yf.download(asset2, start=start, end=end)['Close']
    combined = pd.concat([data1, data2], axis=1, join='outer').dropna()
    combined.columns = ['Asset1', 'Asset2']
    return combined

# --- Model Definition ---
class NeuralSDE(nn.Module):
    def __init__(self, hidden_size, input_features, output_size, lookback):
        super().__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"
        self.lookback = lookback
        self.drift_lstm = nn.LSTM(input_features, hidden_size, batch_first=True)
        self.diffusion_lstm = nn.LSTM(input_features, hidden_size, batch_first=True)
        self.drift_linear = nn.Linear(hidden_size + output_size, output_size)
        self.diffusion_linear = nn.Linear(hidden_size + output_size, output_size)
        self._lstm_hidden_buffer = None

    def f(self, t, y):
        if self._lstm_hidden_buffer is None:
            raise RuntimeError("LSTM hidden state buffer is not set. Call forward with input_sequence first.")
        combined = torch.cat((self._lstm_hidden_buffer, y), dim=1)
        return self.drift_linear(combined)

    def g(self, t, y):
        if self._lstm_hidden_buffer is None:
            raise RuntimeError("LSTM hidden state buffer is not set. Call forward with input_sequence first.")
        combined = torch.cat((self._lstm_hidden_buffer, y), dim=1)
        return self.diffusion_linear(combined)

    def forward(self, ts, y0, input_seq):
        lstm_output, (hidden, _) = self.drift_lstm(input_seq)
        self._lstm_hidden_buffer = hidden.squeeze(0)
        solution = torchsde.sdeint(self, y0, ts, dt=0.01, method='euler')
        self._lstm_hidden_buffer = None
        return solution

# --- Training Routine ---
def train_model(model, train_loader, criterion, optimizer, device, epochs):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device) # Move data to the correct device
            optimizer.zero_grad()
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, inputs.size(1)).to(device) # Move ts to the correct device
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            loss = criterion(preds, targets)
            loss.backward()
            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            running_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}")

# --- Evaluation Routine ---
def evaluate_model(model, test_loader, target_scaler, device, lookback):
    model.eval()
    actual, predicted = [], []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device) # Move data to the correct device
            y0 = inputs[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, inputs.size(1)).to(device) # Move ts to the correct device
            solutions = model(ts, y0, inputs)
            preds = solutions[-1]
            actual.append(targets.squeeze().cpu().numpy()) # Move data back to CPU for numpy
            predicted.append(preds.squeeze().cpu().numpy()) # Move data back to CPU for numpy
    actual = np.concatenate(actual)
    predicted = np.concatenate(predicted)
    actual_orig = target_scaler.inverse_transform(actual.reshape(-1, 1))
    predicted_orig = target_scaler.inverse_transform(predicted.reshape(-1, 1))
    # Align predictions for plotting
    shifted_pred = predicted_orig[:-lookback]
    actual_aligned = actual_orig[lookback:]
    return actual_aligned, shifted_pred

def plot_results(actual, predicted, lookback):
    plt.figure(figsize=(14, 7))
    plt.plot(actual, label='Actual Asset 2 Price')
    plt.plot(predicted, label=f'Predicted Asset 2 Price (Shifted by {lookback} days)')
    plt.title('Actual vs Predicted Prices (Test Set, Shifted)')
    plt.xlabel('Time Steps')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

def independent_test_on_pairs(pairs, features_scaler_combined, target_scaler_combined, model, device):
    # Visualize one pair (the second in the list)
    test_pair_np = pairs[1].cpu().numpy() if torch.is_tensor(pairs[1]) else pairs[1]
    inv_pair = features_scaler_combined.inverse_transform(test_pair_np)

    plt.figure()
    plt.plot(inv_pair[:, 1])
    plt.title("Plot of __[:, 1]")
    plt.figure()
    plt.plot(inv_pair[:, 0])
    plt.title("Plot of __[:, 0]")
    plt.show()

    # Prepare input tensor for model
    test_tensor = torch.tensor(test_pair_np, dtype=torch.float32).unsqueeze(0).to(device)
    y0 = test_tensor[:, 0, 1].unsqueeze(1)
    ts = torch.linspace(0, 1, test_tensor.shape[1]).to(device)

    model.eval()
    with torch.no_grad():
        pred = model(ts, y0, test_tensor)[-1]
        pred_orig = target_scaler_combined.inverse_transform(pred.cpu().numpy().reshape(-1, 1))
        print(pred_orig)
        print("Prediction successful for one pair.")

    predicted_values_all_pairs = []
    actual_values_all_pairs = []

    # Evaluate over all pairs
    model.eval()
    with torch.no_grad():
        for pair in pairs:
            pair = pair.to(device) # Move pair to the correct device
            pair_np = pair.cpu().numpy() if torch.is_tensor(pair) else pair
            test_tensor = torch.tensor(pair_np, dtype=torch.float32).unsqueeze(0).to(device)
            y0 = test_tensor[:, 0, 1].unsqueeze(1)
            ts = torch.linspace(0, 1, test_tensor.shape[1]).to(device)
            pred = model(ts, y0, test_tensor)[-1]
            pred_original = target_scaler_combined.inverse_transform(pred.cpu().numpy().reshape(-1, 1))[0, 0]
            actual_original = features_scaler_combined.inverse_transform(pair_np)[-1, 1]
            predicted_values_all_pairs.append(pred_original)
            actual_values_all_pairs.append(actual_original)
            print(pred_original, actual_original)

    predicted_values_all_pairs = np.array(predicted_values_all_pairs)
    actual_values_all_pairs = np.array(actual_values_all_pairs)
    mean_deviation = np.mean(predicted_values_all_pairs - actual_values_all_pairs)
    print(f"Mean deviation over all pairs: {mean_deviation:.4f}")
    mae = mean_absolute_error(actual_values_all_pairs, predicted_values_all_pairs)
    print(f"Mean Absolute Error over all pairs: {mae:.4f}")

# Example usage in your main function:
# independent_test_on_pairs(pairs, features_scaler_combined, target_scaler_combined, model, device)

# --- Main Execution ---
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("Using device:", device)
    combined_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)
    train_loader, test_loader, feat_scaler, tgt_scaler = preprocess_data(
        combined_data, LOOKBACK, TIME_STEP, BATCH_SIZE, TRAIN_SPLIT
    )
    model = NeuralSDE(HIDDEN_SIZE, INPUT_FEATURES, OUTPUT_SIZE, LOOKBACK).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    train_model(model, train_loader, criterion, optimizer, device, EPOCHS)
    actual, predicted = evaluate_model(model, test_loader, tgt_scaler, device, lookback=14)
    plot_results(actual, predicted, lookback=1)
    print("Processing complete.")

    print("Begin independent testing...")
    # independent_test_on_pairs(pairs, features_scaler_combined, target_scaler_combined, model, device)

# if __name__ == "__main__":
#     main()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)
combined_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)
train_loader, test_loader, feat_scaler, tgt_scaler = preprocess_data(
    combined_data, LOOKBACK, TIME_STEP, BATCH_SIZE, TRAIN_SPLIT
)
model = NeuralSDE(HIDDEN_SIZE, INPUT_FEATURES, OUTPUT_SIZE, LOOKBACK).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
train_model(model, train_loader, criterion, optimizer, device, EPOCHS)
actual, predicted = evaluate_model(model, test_loader, tgt_scaler, device, lookback=1)
plot_results(actual, predicted, lookback=1)
print("Processing complete.")

# Get the last 45 days of data
_ = '2025-06-07'
END_DATE = pd.to_datetime(_) - pd.Timedelta(days=2)
latest_data = load_combined_data(ASSET_1, ASSET_2, start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK)).strftime('%Y-%m-%d'), end=END_DATE)

print(latest_data.head())
# Preprocess the latest data using the same scalers
latest_features = latest_data.values
latest_features_scaled = feat_scaler.transform(latest_features) # Use the already fitted scaler

# Prepare the input for the model
X_latest = np.array([latest_features_scaled[-LOOKBACK:]])
X_latest_tensor = torch.tensor(X_latest, dtype=torch.float32).to(device)

# Make the prediction
model.eval()
with torch.no_grad():
    y0_latest = X_latest_tensor[:, 0, 1].unsqueeze(1)
    ts_latest = torch.linspace(0, 1, X_latest_tensor.size(1)).to(device)
    predicted_scaled = model(ts_latest, y0_latest, X_latest_tensor)[-1]

# Inverse transform the prediction to get the actual price
predicted_price_today = tgt_scaler.inverse_transform(predicted_scaled.cpu().numpy().reshape(-1, 1))

print(f"Predicted price for today: {predicted_price_today[0][0]:.4f}")

END_DATE = pd.to_datetime(_) + pd.Timedelta(days=44)
latest_data = load_combined_data(ASSET_1, ASSET_2, start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK+5)).strftime('%Y-%m-%d'), end=END_DATE)
latest_data

# Simulate a crash in Asset1 (stablecoin) in the latest data
# Calculate the standard deviation of Asset1 in the original combined data
END_DATE = '2025-07-05'

latest_data = load_combined_data(ASSET_1, ASSET_2, start=(pd.to_datetime(END_DATE) - pd.Timedelta(days=LOOKBACK)).strftime('%Y-%m-%d'), end=END_DATE)
asset1_std = latest_data['Asset1'].std()

# Create a copy of the latest data to modify
latest_data_crashed = latest_data.copy()

# Identify the index to introduce the crash (e.g., the last data point in the lookback window)
crash_index = -1 # Introduce the crash at the most recent data point

# Reduce the value of Asset1 at the crash index by 4 standard deviations
latest_data_crashed.iloc[crash_index, latest_data_crashed.columns.get_loc('Asset2')] += -4 * asset1_std

# Preprocess the crashed latest data using the same scalers
latest_features_crashed = latest_data_crashed.values
latest_features_scaled_crashed = feat_scaler.transform(latest_features_crashed) # Use the already fitted scaler

# Prepare the input for the model with the crashed data
X_latest_crashed = np.array([latest_features_scaled_crashed[-LOOKBACK:]])
X_latest_crashed_tensor = torch.tensor(X_latest_crashed, dtype=torch.float32).to(device)

# Make the prediction using the model with the crashed data
model.eval()
with torch.no_grad():
    y0_latest_crashed = X_latest_crashed_tensor[:, 0, 1].unsqueeze(1)
    ts_latest_crashed = torch.linspace(0, 1, X_latest_crashed_tensor.size(1)).to(device)
    predicted_scaled_crashed = model(ts_latest_crashed, y0_latest_crashed, X_latest_crashed_tensor)[-1]

# Inverse transform the prediction to get the actual price under the crash scenario
predicted_price_today_crashed = tgt_scaler.inverse_transform(predicted_scaled_crashed.cpu().numpy().reshape(-1, 1))

print(f"Predicted price for today after stablecoin crash: {predicted_price_today_crashed[0][0]:.4f}")

latest_data_crashed

from statsmodels.tsa.stattools import grangercausalitytests

# Load the combined data again, ensuring we have the original data for the Granger test
granger_data = load_combined_data(ASSET_1, ASSET_2, START_DATE, END_DATE)

# Perform Granger causality test
# The order of columns matters: we are testing if Asset1 causes Asset2
granger_test_results = grangercausalitytests(granger_data[['Asset1', 'Asset2']], maxlag=70, verbose=True)

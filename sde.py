# -*- coding: utf-8 -*-
"""SDE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zaGAlb4VkyedkStHz1iLzgvfDHXtjEGC
"""

!pip install torchsde torchdiffeq
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torchsde
import torch.optim as optim
from torchdiffeq import odeint

def create_lookback_sequence(data, lookback):
    X, y = [], []
    for i in range(len(data) - lookback):
        X.append(data[i:(i + lookback)])
        y.append(data[i + lookback])
    return np.array(X), np.array(y)

def preprocess(ticker, lookback=14):
    data = yf.download(ticker).Close.values

    X, y = create_lookback_sequence(data, lookback)

    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    X_scaler = MinMaxScaler(feature_range=(1,10))
    y_scaler = MinMaxScaler(feature_range=(1,10))

    X_train_reshaped = X_train.reshape(-1, 1)
    X_test_reshaped = X_test.reshape(-1, 1)

    X_train_scaled_reshaped = X_scaler.fit_transform(X_train_reshaped)
    X_test_scaled_reshaped = X_scaler.transform(X_test_reshaped)

    X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)
    X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)

    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))
    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))

    X_train_tensor = torch.tensor(X_train_scaled.reshape(-1, lookback, 1), dtype=torch.float32)
    X_test_tensor = torch.tensor(X_test_scaled[:, :, np.newaxis], dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, X_scaler, y_scaler

train_loader, test_loader, X_scaler, y_scaler = preprocess('USDT-USD')

import torch.nn as nn
import torchsde
import torch

class NeuralSDE(torch.nn.Module):
    def __init__(self, hidden_size, output_size):
        super(NeuralSDE, self).__init__()
        self.noise_type = "diagonal"
        self.sde_type = "ito"


        self.drift_nn = nn.Sequential(
            nn.Linear(output_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

        self.diffusion_nn = nn.Sequential(
            nn.Linear(output_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def f(self, t, y):
        return self.drift_nn(y)

    def g(self, t, y):
        return self.diffusion_nn(y)


    def forward(self, ts, y0):
        return torchsde.sdeint(self, y0, ts, dt=0.01,method = 'euler')

hidden_size = 1024
output_size = 1

model = NeuralSDE(hidden_size, output_size)


print("Neural SDE model defined with f and g methods calling internal networks.")

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, targets in train_loader:
        optimizer.zero_grad()

        y0 = inputs[:, 0, :]

        ts = torch.linspace(0, 1, inputs.shape[1])

        solutions = model(ts, y0)

        predictions = solutions[-1]
        loss = criterion(predictions, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}")

print("Training finished.")

# Set the model to evaluation mode
model.eval()

# Initialize lists to store actual and predicted values
actual_values = []
predicted_values = []

# Iterate over the test data
with torch.no_grad():
    for inputs, targets in test_loader:
        # Initial state for the SDE is the first time step of each sequence
        # Ensure y0 is 2D: (batch_size, 1)
        # Use slicing and squeeze to get shape (batch_size, 1)
        y0 = inputs[:, 0, :].squeeze(1)
        # Time points for the SDE solver
        ts = torch.linspace(0, 1, inputs.shape[1])

        # Forward pass through the Neural SDE model
        solutions = model(ts, y0)

        # Get the prediction from the last time step of the solution
        predictions = solutions[-1]

        # Append actual and predicted values to the lists
        actual_values.append(targets.squeeze().numpy())
        predicted_values.append(predictions.squeeze().numpy())

# Concatenate the lists into numpy arrays
actual_values = np.concatenate(actual_values)
predicted_values = np.concatenate(predicted_values)

print("Prediction generation complete.")

import matplotlib.pyplot as plt
import numpy as np

# Inverse transform the actual and predicted values
# The scaler expects a 2D array, so we need to reshape the 1D actual_values and predicted_values
actual_values_original_scale = y_scaler.inverse_transform(actual_values.reshape(-1, 1))
predicted_values_original_scale = y_scaler.inverse_transform(predicted_values.reshape(-1, 1))

# Shift the predicted values 14 days to the left
# This means aligning the prediction at index i with the actual value at index i - 14
# We will plot predicted_values_original_scale[14:] against actual_values_original_scale[:-14]
shifted_predicted_values = predicted_values_original_scale[14:]
corresponding_actual_values = actual_values_original_scale[:-14]

plt.figure(figsize=(12, 6))
# Plot the corresponding actual values and the shifted predicted values
plt.plot(corresponding_actual_values[:, 0], label='Actual Values (Shifted)')
plt.plot(shifted_predicted_values[:, 0], label='Predicted Values (Shifted Left by 14 Days)')
plt.title('Actual vs Predicted Values (Test Set, Predicted Shifted Left)')
plt.xlabel('Time Steps')
plt.ylabel('Price')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Inverse transform the actual and predicted values
# The scaler expects a 2D array, so we need to reshape the 1D actual_values and predicted_values
actual_values_original_scale = y_scaler.inverse_transform(actual_values.reshape(-1, 1))
predicted_values_original_scale = y_scaler.inverse_transform(predicted_values.reshape(-1, 1))

# Shift the predicted values 14 days to the left
# This means aligning the prediction at index i with the actual value at index i - 14
# We will plot predicted_values_original_scale[14:] against actual_values_original_scale[:-14]
shifted_predicted_values = predicted_values_original_scale[14:]
corresponding_actual_values = actual_values_original_scale[:-14]

plt.figure(figsize=(12, 6))
# Plot the corresponding actual values and the shifted predicted values
plt.plot(corresponding_actual_values[:, 0], label='Actual Values (Shifted)')
plt.plot(shifted_predicted_values[:, 0], label='Predicted Values (Shifted Left by 14 Days)')
plt.title('Actual vs Predicted Values (Test Set, Predicted Shifted Left)')
plt.xlabel('Time Steps')
plt.ylabel('Price')
plt.legend()
plt.show()

!pip install pysr

"""# Task
Perform symbolic regression on the trained drift and diffusion neural networks from the neural SDE model.

## Extract network functions

### Subtask:
Create functions or use the trained `drift_nn` and `diffusion_nn` directly that represent the output of the networks given an input.

**Reasoning**:
Define functions to get the output of the trained drift and diffusion neural networks.
"""

def get_drift_output(y):
    model.eval()
    with torch.no_grad():
        return model.drift_nn(y)

def get_diffusion_output(y):
    model.eval()
    with torch.no_grad():
        return model.diffusion_nn(y)

print("Functions get_drift_output and get_diffusion_output defined.")

"""## Generate data

### Subtask:
Create a dataset of input-output pairs for the symbolic regression model. The input will be the state `y` (and possibly time `t` if it were used by the networks), and the output will be the corresponding output of the `drift_nn` or `diffusion_nn`.

**Reasoning**:
Generate input values covering the expected range of the data and calculate the corresponding drift and diffusion outputs using the previously defined functions.
"""

# Determine the range of scaled data from the training set
min_scaled_y = X_train_scaled.min()
max_scaled_y = X_train_scaled.max()

# Generate a range of input values 'y' within this range
num_points = 1000
y_inputs = torch.linspace(min_scaled_y, max_scaled_y, num_points).unsqueeze(1)

# Calculate the corresponding drift and diffusion outputs
drift_outputs = get_drift_output(y_inputs)
diffusion_outputs = get_diffusion_output(y_inputs)

# Store inputs and outputs
y_inputs_np = y_inputs.squeeze(1).numpy()
drift_outputs_np = drift_outputs.squeeze(1).numpy()
diffusion_outputs_np = diffusion_outputs.squeeze(1).numpy()

print(f"Generated {num_points} input-output pairs for symbolic regression.")

"""**Reasoning**:
The variable `X_train_scaled` was not defined in the current session. I need to re-run the preprocessing step to get the scaled data and the scalers.


"""

def create_lookback_sequence(data, lookback):
    X, y = [], []
    for i in range(len(data) - lookback):
        X.append(data[i:(i + lookback)])
        y.append(data[i + lookback])
    return np.array(X), np.array(y)

def preprocess(ticker, lookback=14):
    data = yf.download(ticker).Close.values

    X, y = create_lookback_sequence(data, lookback)

    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    X_scaler = MinMaxScaler(feature_range=(1,10))
    y_scaler = MinMaxScaler(feature_range=(1,10))

    X_train_reshaped = X_train.reshape(-1, 1)
    X_test_reshaped = X_test.reshape(-1, 1)

    X_train_scaled_reshaped = X_scaler.fit_transform(X_train_reshaped)
    X_test_scaled_reshaped = X_scaler.transform(X_test_reshaped)

    X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)
    X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)

    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))
    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))

    X_train_tensor = torch.tensor(X_train_scaled.reshape(-1, lookback, 1), dtype=torch.float32)
    X_test_tensor = torch.tensor(X_test_scaled[:, :, np.newaxis], dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, X_scaler, y_scaler, X_train_scaled

train_loader, test_loader, X_scaler, y_scaler, X_train_scaled = preprocess('USDT-USD')

"""**Reasoning**:
Now that `X_train_scaled` is available, I can generate the input-output pairs for symbolic regression as originally intended.


"""

# Determine the range of scaled data from the training set
min_scaled_y = X_train_scaled.min()
max_scaled_y = X_train_scaled.max()

# Generate a range of input values 'y' within this range
num_points = 1000
y_inputs = torch.linspace(min_scaled_y, max_scaled_y, num_points).unsqueeze(1)

# Calculate the corresponding drift and diffusion outputs
drift_outputs = get_drift_output(y_inputs)
diffusion_outputs = get_diffusion_output(y_inputs)

# Store inputs and outputs
y_inputs_np = y_inputs.squeeze(1).numpy()
drift_outputs_np = drift_outputs.squeeze(1).numpy()
diffusion_outputs_np = diffusion_outputs.squeeze(1).numpy()

print(f"Generated {num_points} input-output pairs for symbolic regression.")

"""## Perform symbolic regression for drift

### Subtask:
Use PySR or another symbolic regression library to find a symbolic expression that approximates the function of the `drift_nn` based on the generated data.

**Reasoning**:
Import the necessary class and instantiate the PySRRegressor to fit the drift network outputs.
"""

from pysr import PySRRegressor

# Instantiate PySRRegressor for the drift network
# Using y_inputs_np as the single feature
drift_sr = PySRRegressor(
    model_selection='accuracy',
    niterations=100,  # Number of iterations for the search
    binary_operators=["+", "-", "*", "/"],
    unary_operators=["exp", "sin", "cos", "neg", "log", "abs"],
    populations=20,
    ncyclesperiteration=1000,
    npop=500,
    batchsize=500,
    procs=4
)

# Fit the regressor to the drift network data
# Reshape y_inputs_np to be a 2D array (number of samples, number of features)
drift_sr.fit(y_inputs_np.reshape(-1, 1), drift_outputs_np)

# Print the discovered equations
print("Best equations for the drift network:")
print(drift_sr.equations)

"""**Reasoning**:
The previous attempt to instantiate PySRRegressor failed because of an invalid keyword argument 'batchsize'. The traceback suggests using 'batch_size' instead. Correct the keyword and try again.


"""

from pysr import PySRRegressor

# Instantiate PySRRegressor for the drift network
# Using y_inputs_np as the single feature
drift_sr = PySRRegressor(
    model_selection='accuracy',
    niterations=100,  # Number of iterations for the search
    binary_operators=["+", "-", "*", "/"],
    unary_operators=["exp", "sin", "cos", "neg", "log", "abs"],
    populations=20,
    ncycles_per_iteration=1000, # Corrected keyword
    population_size=500, # Corrected keyword
    batch_size=500, # Corrected keyword
    procs=4
)

# Fit the regressor to the drift network data
# Reshape y_inputs_np to be a 2D array (number of samples, number of features)
drift_sr.fit(y_inputs_np.reshape(-1, 1), drift_outputs_np)

# Print the discovered equations
print("Best equations for the drift network:")
print(drift_sr.equations)